from llama_index import ServiceContext

from lib.llm import get_llm
from lib.vector import create_vector
from lib.graph import create_graph
from lib.question_store import store_data
from lib.engine_query import process_queries_with_query_engine

from llama_index.embeddings import FastEmbedEmbedding
from llama_index import set_global_service_context, set_global_handler

def init_rag_process():
    set_global_handler("simple")

urls_mostbauer = [
    'https://mostbauer.com/Poscher',
    'https://mostbauer.com/Jausenstation%20Schned',
    'https://mostbauer.com/Hametner',
    'https://mostbauer.com/Rathwieser',
    'https://mostbauer.com/Willnauer'
]

the_queries_mostbauer = [
    "An welchen Tagen hat der Poscher geöffnet?",
    "Was ist die Adresse von Jausenstation Schned?",
    "Welche Spezialität hat der Schned?",
    "Welche Spezialität hat der Hametner?",
    "Was ist die Telefonnummer von Hametner?",
]

pages_wikipedia_pixel = [
    "Pixel_6",
    "Pixel_6a",
    "Pixel_7",
    "Pixel_7a",
    "Pixel_8"
]
the_queries_wikipedia_pixel = [
    "What is the difference in screen size between Pixel 6 and Pixel 6a?",
    "What is the difference in memory between Pixel 6 and Pixel 6a?",
    "What is the difference in screen size between Pixel 6, Pixel 7 and Pixel 8?",
    "What is the difference in memory between Pixel 6, Pixel 7 and Pixel 8?",
    "Which Pixel Phone has the highest resolution? What is the resolution?"
]
stocknews_stocks = [
    "AY"
]
the_queries_stocknews = [
    "Is the stock AY mentioned rather positively or negatively? Explain your reasoning.",
    "How did the company Atlantica Sustainable Infrastructure plc (AY) do over time ? Explain your reasoning.",
    "Give me an estimated outlook regarding a healthy balance sheet of Atlantica Sustainable Infrastructure plc (AY) ? Explain your reasoning.",
    "Analyse the dividend policy and safety of Atlantica Sustainable Infrastructure plc (AY)! Explain your reasoning."
]
the_queries_zipcode = [
    "How to use method 'run_for_config' ? Which parameters does it take ?",
    "What does the method 'create_vector' specifically do ? Explain step by step. Focus on a higher level of semantic rather than syntax and line by line description.",
    "Describe in short what the whole project is about. Do it step by step. Focus on an overview of the whole project.",
]

def run_for_config(embed_model_name, llm_engine, llm_model, openai_model, run_identifier, be_verbose=False):
    """LlamaIndex_Simple_Graph_RAG.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/github/cgint/llama_index_t1/blob/main/LlamaIndex_Simple_Graph_RAG.ipynb
    """
    questions_data_out_file = f"/data/questions_{run_identifier}.csv"
    
    document_store_path = f"/data/input_{run_identifier}_md.txt"
    # urlPostfix = "?format=markdown2"
    # doc_loader = lambda: load_documents(urls_mostbauer, urlPostfix, document_store_path)
    # doc_loader = lambda: load_documents_wikipedia(pages_wikipedia_pixel, document_store_path)
    # doc_loader = lambda: load_documents_stocknews(stocknews_stocks, document_store_path)
    from lib.loader import load_documents_from_files_in_zipfile
    doc_loader = lambda: load_documents_from_files_in_zipfile("/data/llama_index_t1_base.zip", document_store_path)
    the_queries = the_queries_zipcode

    # print("About to instanciate phoenix ...")
    # from llama_index.callbacks import CallbackManager
    # import phoenix as px
    # from phoenix.trace.llama_index import OpenInferenceTraceCallbackHandler
    # session = px.launch_app()
    # trace_handler = OpenInferenceTraceCallbackHandler()

    llm = get_llm(llm_engine, llm_model, openai_model)
    print(f"About to instanciate Embed Model {embed_model_name} using FastEmbedEmbedding ...")
    
    embed_model = FastEmbedEmbedding(model_name=embed_model_name, cache_dir="/data/fastembed_cache/")
    service_context = ServiceContext.from_defaults(
        llm=llm, 
        chunk_size=512, 
        embed_model=embed_model, # embed_model="local"
        # callback_manager=CallbackManager(handlers=[trace_handler])
    )
    set_global_service_context(service_context)

    questions_data = []

    context = "Vector RAG Query Engine"
    print("====================================")
    print(f"     {context}")
    print("====================================")

    ## Build Vector-Index
    vector_storage_dir = f'/data/storage_vector_{run_identifier}'
    vector_index = create_vector(service_context, vector_storage_dir, lambda: doc_loader())

    vector_rag_query_engine = vector_index.as_query_engine()

    questions_data.extend(process_queries_with_query_engine(vector_rag_query_engine, context, the_queries))

    store_data(questions_data, questions_data_out_file)

    context = "SubQuestion Query Engine with Vector RAG Tool"
    print("====================================")
    print(f"     {context}")
    print("====================================")
    from llama_index.tools import QueryEngineTool
    from llama_index.query_engine import SubQuestionQueryEngine
    from llama_index.question_gen.guidance_generator import GuidanceQuestionGenerator
    question_gen = GuidanceQuestionGenerator.from_defaults(guidance_llm=llm, verbose=True)

    sub_question_query_engine = SubQuestionQueryEngine.from_defaults(
        query_engine_tools=[
            QueryEngineTool.from_defaults(
                query_engine=vector_rag_query_engine,
                description="Source code questions are about."
            )
        ],
        question_gen=question_gen
    )

    questions_data.extend(process_queries_with_query_engine(sub_question_query_engine, context, the_queries))

    store_data(questions_data, questions_data_out_file)

    context = "Vector RAG MultiStep Query Engine"
    print("====================================")
    print(f"     {context}")
    print("====================================")
    from llama_index.query_engine.multistep_query_engine import MultiStepQueryEngine
    from llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform
    multistep_query_engine = MultiStepQueryEngine(
        vector_rag_query_engine,
        query_transform=StepDecomposeQueryTransform(llm=llm, verbose=be_verbose),
        index_summary = "Used to answer questions about the code"
    )

    questions_data.extend(process_queries_with_query_engine(multistep_query_engine, context, the_queries))

    store_data(questions_data, questions_data_out_file)

    context = "QASummaryQueryEngine"
    print("====================================")
    print(f"     {context}")
    print("====================================")

    from llama_index.composability.joint_qa_summary import QASummaryQueryEngineBuilder
    # NOTE: can also specify an existing docstore, service context, summary text, qa_text, etc.
    qa_doc_loader = doc_loader
    print(f"About to build QASummaryQueryEngine over {len(qa_doc_loader())} document(s) ...")
    query_engine_builder = QASummaryQueryEngineBuilder(
        service_context=service_context
    )
    qa_summary_query_engine = query_engine_builder.build_from_documents(qa_doc_loader())
    print(f"Building QASummaryQueryEngine done. Asking questions now ...")
    questions_data.extend(process_queries_with_query_engine(qa_summary_query_engine, context, the_queries))

    store_data(questions_data, questions_data_out_file)
    # px.active_session().url
    
    # return
    ## Build Graph-Index
    kg_index_storage_dir = f'/data/storage_graph_{run_identifier}'
    kg_index = create_graph(service_context, kg_index_storage_dir, f"/data/example_{run_identifier}.html", lambda: doc_loader())

    context = "KG_Index as Query Engine"
    print("====================================")
    print(f"     {context}")
    print("====================================")

    kg_index_query_engine = kg_index.as_query_engine(
        retriever_mode="hybrid",
        verbose=be_verbose,
        response_mode="tree_summarize",
        similarity_top_k=5,
        include_text=True,
        graph_store_query_depth=6,
    )
    questions_data.extend(process_queries_with_query_engine(kg_index_query_engine, context, the_queries))

    store_data(questions_data, questions_data_out_file)

    context = "RouterQueryEngine with tools: KG_Index as Query Engine, Vector RAG Query Engine"
    print("====================================")
    print(f"     {context}")
    print("====================================")

    from llama_index.tools.query_engine import QueryEngineTool
    keyword_tool = QueryEngineTool.from_defaults(
        query_engine=kg_index_query_engine,
        description="Useful for answering questions about relationships"
    )

    vector_tool = QueryEngineTool.from_defaults(
        query_engine=vector_rag_query_engine,
        description="Useful for answering questions about semantic similarity"
    )

    from llama_index.query_engine.router_query_engine import RouterQueryEngine
    from llama_index.selectors.llm_selectors import (
        LLMSingleSelector,
        LLMMultiSelector,
    )
    from llama_index.selectors.pydantic_selectors import (
        PydanticMultiSelector,
        PydanticSingleSelector,
    )
    from llama_index.response_synthesizers import TreeSummarize
    from llama_index.prompts import PromptTemplate

    TREE_SUMMARIZE_PROMPT_TMPL = (
        "Context information from multiple sources is below. Each source may or"
        " may not have \na relevance score attached to"
        " it.\n---------------------\n{context_str}\n---------------------\nGiven"
        " the information from multiple sources and their associated relevance"
        " scores (if provided) and not prior knowledge, answer the question. If"
        " the answer is not in the context, inform the user that you can't answer"
        " the question.\nQuestion: {query_str}\nAnswer: "
    )

    tree_summarize = TreeSummarize(
        summary_template=PromptTemplate(TREE_SUMMARIZE_PROMPT_TMPL)
    )

    router_query_engine_tools = RouterQueryEngine(
        selector=LLMMultiSelector.from_defaults(),
        query_engine_tools=[
            keyword_tool,
            vector_tool,
        ],
        summarizer=tree_summarize,
    )

    questions_data.extend(process_queries_with_query_engine(router_query_engine_tools, context, the_queries))

    store_data(questions_data, questions_data_out_file)

    context = "SubQuestion Query Engine with tools: KG_Index as Query Engine, Vector RAG Query Engine"
    print("====================================")
    print(f"     {context}")
    print("====================================")
    sub_question_query_engine_tools = SubQuestionQueryEngine.from_defaults(
        query_engine_tools=[
            keyword_tool,
            vector_tool,
        ],
        question_gen=question_gen
    )

    questions_data.extend(process_queries_with_query_engine(sub_question_query_engine_tools, context, the_queries))

    store_data(questions_data, questions_data_out_file)

    context = "MultiStep Query Engine based on RouterQueryEngine with tools vector and graph"
    print("====================================")
    print(f"     {context}")
    print("====================================")
    from llama_index.query_engine.multistep_query_engine import MultiStepQueryEngine
    from llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform
    multistep_router_query_engine = MultiStepQueryEngine(
        router_query_engine_tools,
        query_transform=StepDecomposeQueryTransform(llm=llm, verbose=be_verbose),
        index_summary = "Used to answer questions about the code"
    )

    questions_data.extend(process_queries_with_query_engine(multistep_router_query_engine, context, the_queries))

    store_data(questions_data, questions_data_out_file)

    """See also here for comparison of text2cypher & GraphRAG
    - https://user-images.githubusercontent.com/1651790/260617657-102d00bc-6146-4856-a81f-f953c7254b29.mp4
    - https://siwei.io/en/demos/text2cypher/

    > While another idea is to retrieve in both ways and combine the context to fit more use cases.

    ### Graph RAG on any existing KGs

    with `KnowledgeGraphRAGRetriever`.

    REF: https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html#perform-graph-rag-query
    """

    context = "KnowledgeGraphRAGRetriever"
    print("====================================")
    print(f"     {context}")
    print("====================================")

    from llama_index.query_engine import RetrieverQueryEngine
    from llama_index.retrievers import KnowledgeGraphRAGRetriever

    graph_rag_retriever = KnowledgeGraphRAGRetriever(
        storage_context=kg_index.storage_context,
        service_context=service_context,
        llm=llm,
        verbose=be_verbose,
    )

    lg_rag_ret_query_engine = RetrieverQueryEngine.from_args(
        graph_rag_retriever, service_context=service_context
    )
    questions_data.extend(process_queries_with_query_engine(lg_rag_ret_query_engine, context, the_queries))

    store_data(questions_data, questions_data_out_file)
    
