{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgint/llama_index_t1/blob/main/LlamaIndex_Simple_Graph_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Graph LLM, Demo Outline\n",
        "\n",
        "Got it from [Graph_RAG_LlamaIndex_Workshop.ipynb](https://colab.research.google.com/drive/1tLjOg2ZQuIClfuWrAC2LdiZHCov8oUbs#scrollTo=s5LPkzt1YUIN)\n",
        "\n"
      ],
      "metadata": {
        "id": "iDjEGsguhCzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Graph RAG\n",
        "\n",
        "> Graph RAG with LLM\n",
        "\n",
        "![Graph RAG](https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870)\n",
        "\n",
        "\n",
        "> Query time workflow:\n",
        "\n",
        "- Get Key Entities/Relationships related to task\n",
        "  - LLM or NLP to extract from task string\n",
        "  - Expand synonyms\n",
        "- Get SubGraphs\n",
        "  - Exact matching of \"Starting Point\"\n",
        "  - Optionally Embedding based\n",
        "- Generate answer based on SubGraphs\n",
        "  - Could be combined with other RAG\n",
        "  - If KG was built with LlamaIndex, metadata could be leveraged\n",
        "\n",
        "> Values\n",
        "\n",
        "- KG is __ of Knowledge:\n",
        "  - Refined and Concise Form\n",
        "  - Fine-grained Segmentation\n",
        "  - Interconnected-structured nature\n",
        "- Knowledge in (existing) KG is Accurate\n",
        "- Query towards KG is Stable yet Deterministic\n",
        "- Reasoning/Info. in KG persist domain knowledge\n",
        "\n",
        "> Refs:\n",
        "\n",
        "- https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html\n",
        "- https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html\n",
        "- https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html\n",
        "- https://siwei.io/talks/graph-rag-with-jerry/\n",
        "- https://www.youtube.com/watch?v=bPoNCkjDmco"
      ],
      "metadata": {
        "id": "AQp9z4hRqNpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KG Building\n",
        "\n",
        "![KG Building](https://github.com/siwei-io/talks/assets/1651790/495e035e-7975-4b77-987a-26f8e1d763d2)\n",
        "\n",
        "> Value\n",
        "\n",
        "- Game-changer for ROI on adaptation of Graph\n",
        "  - NLP Competence and efforts\n",
        "  - Complex Pipelines\n",
        "- Those \"nice to have\" graphs can now be enabled by Graph at a small cost\n",
        "\n",
        "> Refs\n",
        "- https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.html#instantiate-gptnebulagraph-kg-indexes\n",
        "- https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_query_engine.html\n",
        "- https://colab.research.google.com/drive/1G6pcR0pXvSkdMQlAK_P-IrYgo-_staxd?usp=sharing\n",
        "- https://siwei.io/en/demos/text2cypher/\n",
        "- https://siwei.io/demo-dumps/kg-llm/KG_Building.ipynb\n",
        "- https://siwei.io/demo-dumps/kg-llm/KG_Building.html"
      ],
      "metadata": {
        "id": "X45JFcoqqO_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How\n",
        "\n",
        "with LlamaIndex and Simple Graph in memory"
      ],
      "metadata": {
        "id": "597Ejvia1tn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concepts\n",
        "\n",
        "REF: https://gpt-index.readthedocs.io/en/stable/getting_started/concepts.html\n",
        "\n",
        "### RAG\n",
        "\n",
        "Retrieval Augmented Generation:\n",
        "\n",
        "![](https://gpt-index.readthedocs.io/en/stable/_images/rag.jpg)\n",
        "\n",
        "### Indexing Stage\n",
        "\n",
        "![](https://gpt-index.readthedocs.io/en/stable/_images/indexing.jpg)\n",
        "- [Data Connectors(LlamaHub)](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/connector/root.html)\n",
        "\n",
        "- Documents\n",
        "  - Nodes(Chunk)\n",
        "- Index\n",
        "  - VectorIndex\n",
        "  - [KnowledgeGraphIndex](https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html), create KG from data, Graph RAG\n",
        "  - SQLIndex\n",
        "\n",
        "\n",
        "### Querying Stage\n",
        "\n",
        "- Query Engine/Chat Engine Agent(input text, output answer)\n",
        "  - [KnowledgeGraphQueryEngine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_query_engine.html), Text2Cypher Query engine\n",
        "- Retriever(input text, output nodes)\n",
        "  - [KnowledgeGraphRAGRetriever](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html), for existing KG wired as Graph RAG\n",
        "- Node Postprocessor(Reranking, filterring nodes)\n",
        "- Response Synthesizer(input nodes, output answer)\n",
        "\n",
        "![](https://gpt-index.readthedocs.io/en/stable/_images/querying.jpg)\n",
        "\n",
        "\n",
        "### Context\n",
        "\n",
        "REF:\n",
        "- https://gpt-index.readthedocs.io/en/stable/core_modules/supporting_modules/service_context.html\n",
        "- https://gpt-index.readthedocs.io/en/stable/api_reference/storage.html\n",
        "\n",
        "\n",
        "\n",
        "Service context\n",
        "\n",
        "- LLM\n",
        "- Embedding Model\n",
        "- Prompt Helper\n",
        "\n",
        "Storage context\n",
        "\n",
        "- Vector Store\n",
        "- Graph Store\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JaWbX94e16hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key KG related components\n",
        "\n",
        "- [KnowledgeGraphIndex](https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html) is an Index to:\n",
        "  - Indexing stage:\n",
        "    - Extract data into KG with LLM or any other callable models\n",
        "    - Persist KG data into `storeage_context.graph_store`\n",
        "  - Querying stage:\n",
        "    - `as_query_engine()` to enable 0-shot Graph RAG\n",
        "    - `as_retriever()` to create an advanced Graph involving RAG\n",
        "- [KnowledgeGraphRAGRetriever](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html)\n",
        "  - Instanctiate:\n",
        "    - Create a `storeage_context.graph_store` as the init argument.\n",
        "  - Querying stage:\n",
        "    - pass to `RetrieverQueryEngine` to become a Graph RAG query engine on any existing KG\n",
        "    - combined with other RAG pipeline\n",
        "\n",
        "- [KnowledgeGraphQueryEngine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_query_engine.html), Text2Cypher Query engine\n",
        "  - Instanctiate:\n",
        "    - Create a `storeage_context.graph_store` as the init argument.\n",
        "  - Querying stage:\n",
        "    - Text2cypher to get answers towards the KG in graph_store.\n",
        "    - Optionally, `generate_query()` only compose a cypher query."
      ],
      "metadata": {
        "id": "_GEZj4Pg8bjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "\n",
        "Install Dependencies, prepare for contexts of Llama Index"
      ],
      "metadata": {
        "id": "s5LPkzt1YUIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai llama_index pyvis"
      ],
      "metadata": {
        "id": "mdEYgeyeT7aI",
        "outputId": "1371b6e7-3427-45cb-e8ef-07d730bbd817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Using cached openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "Collecting llama_index\n",
            "  Downloading llama_index-0.9.24-py3-none-any.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyvis\n",
            "  Using cached pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.9.1)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from llama_index)\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from llama_index)\n",
            "  Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2023.6.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama_index)\n",
            "  Using cached tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.2)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.2.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama_index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama_index) (1.14.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama_index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama_index)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2023.3.post1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
            "Installing collected packages: typing-extensions, mypy-extensions, marshmallow, jedi, h11, deprecated, beautifulsoup4, typing-inspect, tiktoken, httpcore, pyvis, httpx, dataclasses-json, openai, llama_index\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beautifulsoup4-4.12.2 dataclasses-json-0.6.3 deprecated-1.2.14 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jedi-0.19.1 llama_index-0.9.24 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.6.1 pyvis-0.3.2 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-xxx\"\n",
        "```"
      ],
      "metadata": {
        "id": "fxH99CEo7_ml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iDA3lAm0LatM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Read credentials\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Define the command to source the openrc file and print environment variables\n",
        "source_command = 'bash -c \". openrc && env\"'\n",
        "\n",
        "# Run the command and capture its output\n",
        "completed_process = subprocess.run(source_command, shell=True, stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "# Parse the output to extract environment variables\n",
        "env_output = completed_process.stdout\n",
        "env_lines = env_output.splitlines()\n",
        "env_variables = {}\n",
        "\n",
        "for line in env_lines:\n",
        "    key, value = line.split('=', 1)\n",
        "    if any([\n",
        "        \"OPENAI\" in key\n",
        "    ]):\n",
        "        env_variables[key] = value\n",
        "\n",
        "os.environ.update(env_variables)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For OpenAI\n",
        "\n",
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"], handled in openrc reading\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(\n",
        "    stream=sys.stdout, level=logging.INFO\n",
        ")\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    KnowledgeGraphIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "from llama_index.graph_stores import NebulaGraphStore\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "\n",
        "# define LLM\n",
        "llm = OpenAI(temperature=0, model=\"<model_name>\")\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
        "\n",
        "# set global service context\n",
        "set_global_service_context(service_context)"
      ],
      "metadata": {
        "id": "x_rIydAQQTT7",
        "outputId": "801d0cc9-4b61-4987-8698-9d2f58e0bea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c5a468c8e199>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from llama_index import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mSimpleDirectoryReader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Graph Space\n",
        "\n",
        "KnowledgeGraphIndex on SimpleGraphStore\n"
      ],
      "metadata": {
        "id": "kkHpLzEuYo_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storage_context with Graph_Store"
      ],
      "metadata": {
        "id": "MgOeUJ3QbfoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_store = SimpleGraphStore()\n",
        "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
      ],
      "metadata": {
        "id": "3ArMP3vrBDUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è KG Building with Llama Index"
      ],
      "metadata": {
        "id": "3yWYQzpq9UWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Data with data connectors\n",
        "\n",
        "with `WikipediaReader`\n",
        "\n",
        "We will download and preprecess data from:\n",
        "    https://en.wikipedia.org/wiki/Guardians_of_the_Galaxy_Vol._3"
      ],
      "metadata": {
        "id": "H9D9oVyBGdXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "\n",
        "loader = WikipediaReader()\n",
        "\n",
        "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
      ],
      "metadata": {
        "id": "5t30TRBkGadF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing Extract Triplets and Save to NebulaGraph\n",
        "\n",
        "with `KnowledgeGraphIndex`\n",
        "\n",
        "This call will take some time, it'll extract entities and relationships and store them into NebulaGraph"
      ],
      "metadata": {
        "id": "fzP47o2ZGgev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kg_index = KnowledgeGraphIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        "    service_context=service_context,\n",
        "    max_triplets_per_chunk=10,\n",
        "    include_embeddings=True\n",
        ")"
      ],
      "metadata": {
        "id": "LkxaqAotGmbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßô Text2Cypher"
      ],
      "metadata": {
        "id": "bTWsuvyT81PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine import KnowledgeGraphQueryEngine\n",
        "\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "from llama_index.graph_stores import NebulaGraphStore\n",
        "\n",
        "nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
        "    storage_context=storage_context,\n",
        "    service_context=service_context,\n",
        "    llm=lc_llm,\n",
        ")"
      ],
      "metadata": {
        "id": "7d2gXzA_ClcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate connections\n",
        "%ngql SHOW HOSTS\n",
        "r = nl2kg_query_engine.query(\"SHOW HOSTS\")"
      ],
      "metadata": {
        "id": "G8Ujutz8-vCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Tell me about Rocket?\n",
        "\"\"\"\n",
        "\n",
        "response_nl2kg = nl2kg_query_engine.query(question)\n",
        "\n",
        "# Cypher:\n",
        "\n",
        "print(\"The Cypher Query is:\")\n",
        "\n",
        "query_string = nl2kg_query_engine.generate_query(question)\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "```cypher\n",
        "{query_string}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")\n",
        "\n",
        "%ngql {query_string}\n",
        "\n",
        "# Answer:\n",
        "\n",
        "print(\"The Answer is:\")\n",
        "\n",
        "display(Markdown(f\"<b>{response_nl2kg}</b>\"))"
      ],
      "metadata": {
        "id": "QvRXjA5YcHu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "What challenges do Rocket and Lylla face?\n",
        "\"\"\"\n",
        "\n",
        "response_nl2kg = nl2kg_query_engine.query(question)\n",
        "\n",
        "# Cypher:\n",
        "\n",
        "print(\"The Cypher Query is:\")\n",
        "\n",
        "query_string = nl2kg_query_engine.generate_query(question)\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "```cypher\n",
        "{query_string}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")\n",
        "\n",
        "%ngql {query_string}\n",
        "\n",
        "# Answer:\n",
        "\n",
        "print(\"The Answer is:\")\n",
        "\n",
        "display(Markdown(f\"<b>{response_nl2kg}</b>\"))"
      ],
      "metadata": {
        "id": "qq5dTy4HC45i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valina LLM + Prompts doesn't work well on all questions, fine-tuning, or few-shot ways could push further.\n",
        "\n",
        "But Graph RAG is easier as:\n",
        "- The query-composing doesn't rely on the higher intelligence\n",
        "- Easier to enable approximate starting entities\n",
        "- Easier to push CoT-like task-break-down in the orchestration layer\n"
      ],
      "metadata": {
        "id": "HUjYQZ8D_U8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Graph RAG\n",
        "\n"
      ],
      "metadata": {
        "id": "XLK-uhfxdZZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KG_Index as **Query Engine**"
      ],
      "metadata": {
        "id": "h-RxD8fnAtiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kg_index_query_engine = kg_index.as_query_engine(\n",
        "    retriever_mode=\"keyword\",\n",
        "    verbose=True,\n",
        "    response_mode=\"tree_summarize\",\n",
        ")"
      ],
      "metadata": {
        "id": "yDXW15OrHo7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_graph_rag = kg_index_query_engine.query(\"What challenges do Rocket and Lylla face?\")\n",
        "\n",
        "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
      ],
      "metadata": {
        "id": "84nzpNb_A2GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_graph_rag = kg_index_query_engine.query(\"Tell me about James Gunn.\")\n",
        "\n",
        "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
      ],
      "metadata": {
        "id": "3RjL-mkaHvjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ngql USE rag_workshop; MATCH p=(n)-[e:relationship*1..2]-() WHERE id(n) in ['James Gunn', 'James', 'Gunn'] RETURN p"
      ],
      "metadata": {
        "id": "s3vW6sFxemD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ng_draw"
      ],
      "metadata": {
        "id": "3mTyrfkxTJBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Tell me about James Gunn.\n",
        "\"\"\"\n",
        "\n",
        "response_nl2kg = nl2kg_query_engine.query(question)\n",
        "\n",
        "# Cypher:\n",
        "\n",
        "print(\"The Cypher Query is:\")\n",
        "\n",
        "query_string = nl2kg_query_engine.generate_query(question)\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "```cypher\n",
        "{query_string}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")\n",
        "\n",
        "%ngql {query_string}\n",
        "\n",
        "# Answer:\n",
        "\n",
        "print(\"The Answer is:\")\n",
        "\n",
        "display(Markdown(f\"<b>{response_nl2kg}</b>\"))"
      ],
      "metadata": {
        "id": "Hn1VWjb3Blqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%ngql\n",
        "MATCH p=(e1:`entity`)-[r:`relationship`]->(e2:`entity`)\n",
        "WHERE e1.`entity`.`name` == 'James Gunn'\n",
        "RETURN p"
      ],
      "metadata": {
        "id": "IWluYtm1Bv3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ng_draw"
      ],
      "metadata": {
        "id": "rhR4Er9PB8mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "See also here for comparison of text2cypher & GraphRAG\n",
        "- https://user-images.githubusercontent.com/1651790/260617657-102d00bc-6146-4856-a81f-f953c7254b29.mp4\n",
        "- https://siwei.io/en/demos/text2cypher/\n",
        "\n",
        "> While another idea is to retrieve in both ways and combine the context to fit more use cases.\n"
      ],
      "metadata": {
        "id": "vqFXnKJ2CMBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph RAG on any existing KGs\n",
        "\n",
        "with `KnowledgeGraphRAGRetriever`.\n",
        "\n",
        "REF: https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html#perform-graph-rag-query"
      ],
      "metadata": {
        "id": "KYAacH50C-1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.retrievers import KnowledgeGraphRAGRetriever\n",
        "\n",
        "graph_rag_retriever = KnowledgeGraphRAGRetriever(\n",
        "    storage_context=storage_context,\n",
        "    service_context=service_context,\n",
        "    llm=lc_llm,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    graph_rag_retriever, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "1xoT_NIMTLgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Who is Rocket?\",\n",
        ")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "EniVfVn5U3fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Graph RAG Chat Engine"
      ],
      "metadata": {
        "id": "ajXcbqbSh54Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The context mode"
      ],
      "metadata": {
        "id": "aKrJ-P0bl7k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.memory import ChatMemoryBuffer\n",
        "\n",
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
        "\n",
        "chat_engine = kg_index.as_chat_engine(\n",
        "    chat_mode=\"context\",\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "response = chat_engine.chat(\"who is Rocket?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"who is Lylla?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"who is Groot?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"do they all know each other?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "CgzkbXu6hQeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"But how about Lylla?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "gtS2JMOykR8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above chat_engine won't eval the \"them\" when doing RAG, this could be resolved with ReAct mode!\n",
        "\n",
        "We can see, now the agent will refine the question towards RAG before the retrieval."
      ],
      "metadata": {
        "id": "luOmU0Hzkaft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The ReAct mode"
      ],
      "metadata": {
        "id": "wz_OaiWhl_3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
        "\n",
        "chat_engine = kg_index.as_chat_engine(\n",
        "    chat_mode=\"react\",\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "response = chat_engine.chat(\"who is Rocket?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"who is Lylla?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"who is Groot?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"who of them are human?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "kXm1o2AOkc7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Styling the chatbot, like... a rapper?\n",
        "\n",
        "> prompt was [composed based on examples from Llama Index](https://shareg.pt/hKA9Yld)."
      ],
      "metadata": {
        "id": "vYprhy09rVf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
        "\n",
        "prompt_as_a_rapper = \"\"\"\n",
        "You are a freestyle rap assistant who speaks in the fluid, rhythmic style of hip-hop. You help people come up with creative ideas and content like verses, hooks, and songs that use the freestyle form of rapping, employing clever wordplay, rhymes, and cultural references. Here are some examples of a freestyle style:\n",
        "\n",
        "\"Life's a game but it's not fair, I break the rules so I don't care.\"\n",
        "\"From the concrete who knew that a flower would grow?\"\n",
        "\"Mic check one-two, coming through with the crew, words like a maze, catching the groove as they move.\"\n",
        "\"\"\"\n",
        "\n",
        "chat_engine = kg_index.as_chat_engine(\n",
        "    chat_mode=\"context\",\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    system_prompt=prompt_as_a_rapper,\n",
        ")\n",
        "response = chat_engine.chat(\"who is Rocket?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"who is Lylla?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "response = chat_engine.chat(\"who is Groot?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "7jpk8KHQpzMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs:\n",
        "- https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/\n",
        "- https://github.com/wey-gu/demo-kg-build/blob/main/graph_rag_chatbot.py\n",
        "- https://llamaindex-chat-with-docs.streamlit.app/"
      ],
      "metadata": {
        "id": "Pn6KZzlHSAeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<iframe src=\"https://player.vimeo.com/video/857919385?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479\" width=\"1080\" height=\"525\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" title=\"chat_graph_rag_demo\"></iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "6FkfO8pHTJ-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph RAG with Text2Cypher"
      ],
      "metadata": {
        "id": "XBF7lMRjDl_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(\n",
        "    storage_context=storage_context,\n",
        "    service_context=service_context,\n",
        "    llm=lc_llm,\n",
        "    verbose=True,\n",
        "    with_nl2graphquery=True,\n",
        ")\n",
        "\n",
        "query_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(\n",
        "    graph_rag_retriever_with_nl2graphquery, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "Ky0GgEPzCYPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine_with_nl2graphquery.query(\"Tell me about Rocket?\")\n",
        "\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "An32fcNZC4hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Graph RAG and Vector Index\n",
        "\n",
        "REF: https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html\n",
        "\n",
        "```\n",
        "                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  \n",
        "                  ‚îÇ 1  ‚îÇ 2  ‚îÇ 3  ‚îÇ 4  ‚îÇ                  \n",
        "                  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î§                  \n",
        "                  ‚îÇ  Docs/Knowledge   ‚îÇ                  \n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ        ...        ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ       ‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ         ‚îÇ\n",
        "‚îÇ       ‚îÇ         ‚îÇ 95 ‚îÇ 96 ‚îÇ    ‚îÇ    ‚îÇ       ‚îÇ         ‚îÇ\n",
        "‚îÇ       ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ         ‚îÇ\n",
        "‚îÇ User  ‚îÇ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ‚ñ∂   LLM   ‚îÇ\n",
        "‚îÇ       ‚îÇ                                     ‚îÇ         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îå ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚ñ≤     \n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂‚îÇ  Tell me ....., please   ‚îÇ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     \n",
        "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              \n",
        "           ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ             \n",
        "            ‚îÇ 3  ‚îÇ ‚îÇ 96 ‚îÇ x->y, x<-z->b,..               \n",
        "           ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ             \n",
        "            ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ\n",
        "```"
      ],
      "metadata": {
        "id": "3esi2L3lENRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector Index creation"
      ],
      "metadata": {
        "id": "BjRV_pTKK1Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "rstZX39SJp6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_rag_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "mjlIp72fKHUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Persist and restore"
      ],
      "metadata": {
        "id": "iM-MHkNoKvnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vector_index.storage_context.persist(persist_dir='./storage_vector')"
      ],
      "metadata": {
        "id": "sA_EvbtaJ6ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import load_index_from_storage\n",
        "\n",
        "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
        "vector_index = load_index_from_storage(\n",
        "    service_context=service_context,\n",
        "    storage_context=storage_context_vector\n",
        ")"
      ],
      "metadata": {
        "id": "FkvEr6edKtWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Cherry-picked\" Examples that KG helps\n"
      ],
      "metadata": {
        "id": "2NJBjvBOuogO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Top-K Retrieval, nature of information distribution and segmentation\n",
        "\n",
        "See more from [here](https://siwei.io/graph-enabled-llama-index/kg_and_vector_RAG.html).\n",
        "\n",
        "> Tell me events about NASA.\n",
        "\n",
        "|        | VectorStore                                                  | Knowledge Graph + VectorStore                                | Knowledge Graph                                              |\n",
        "| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| Answer | NASA scientists report evidence for the existence of a second Kuiper Belt,<br>which the New Horizons spacecraft could potentially visit during the late 2020s or early 2030s.<br>NASA is expected to release the first study on UAP in mid-2023.<br>NASA's Venus probe is scheduled to be launched and to arrive on Venus in October,<br>partly to search for signs of life on Venus.<br>NASA is expected to start the Vera Rubin Observatory, the Qitai Radio Telescope,<br>the European Spallation Source and the Jiangmen Underground Neutrino.<br>NASA scientists suggest that a space sunshade could be created by mining the lunar soil and<br> launching it towards the Sun to form a shield against global warming. | NASA announces future space telescope programs on May 21.<br>**NASA publishes images of debris disk on May 23. NASA discovers exoplanet LHS 475 b on May 25.**<br>NASA scientists present evidence for the existence of a second Kuiper Belt on May 29.<br>NASA confirms the start of the next El Ni√±o on June 8.<br>NASA produces the first X-ray of a single atom on May 31.<br>NASA reports the first successful beaming of solar energy from space down to a receiver on the ground on June 1.<br>NASA scientists report evidence that Earth may have formed in just three million years on June 14.<br>NASA scientists report the presence of phosphates on Enceladus, moon of the planet Saturn, on June 14.<br>NASA's Venus probe is scheduled to be launched and to arrive on Venus in October.<br>NASA's MBR Explorer is announced by the United Arab Emirates Space Agency on May 29.<br>NASA's Vera Rubin Observatory is expected to start in 2023. | NASA announced future space telescope programs in mid-2023,<br>**published images of a debris disk**, <br>and discovered an exoplanet called **LHS 475 b**. |\n",
        "| Cost   | 1897 tokens                                                  | 2046 Tokens                                                  | 159 Tokens                                                   |\n",
        "\n",
        "\n",
        "\n",
        "And we could see there are indeed some knowledges added with the help of Knowledge Graph retriever:\n",
        "\n",
        "- NASA publishes images of debris disk on May 23.\n",
        "- NASA discovers exoplanet LHS 475 b on May 25.\n",
        "\n",
        "The additional cost, however, does not seem to be very significant, at `7.28%`: `(2046-1897)/2046`.\n",
        "\n",
        "Furthermore, the answer from the knwoledge graph is extremely concise (only 159 tokens used!), but is still informative.\n",
        "\n",
        "> Takeaway: KG gets Fine-grained Segmentation of info. with the nature of interconnection/global-context-retained, it helps when retriving spread yet important knowledge pieces.\n"
      ],
      "metadata": {
        "id": "3g6agniJOudp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hallucination due to w/ relationship in literal/common sense, but should not be connected in domain Knowledge\n",
        "\n",
        "[GPT-4 (WebPilot) helped me](https://shareg.pt/4zbGI5G) construct this question:\n",
        "\n",
        "> during their mission on Counter-Earth, the Guardians encounter a mysterious artifact known as the 'Celestial Compass', said to be a relic from Star-Lord's Celestial lineage. Who among the Guardians was tempted to use its power for personal gain?\n",
        "\n",
        "where, the correlation between knowledge/documents were setup in \"common sence\", while, they shouldn't be linked as in domain knowledge.\n",
        "\n",
        "See this picture, they could be considered related w/o knowing they shouldn't be categorized together in the sense of e-commerce.\n",
        "\n",
        "> Insulated Greenhouse v.s. Insulated Cup\n",
        "<div style=\"display: flex; justify-content: space-between;\">\n",
        "    <img src=\"https://github.com/siwei-io/talks/assets/1651790/81ff9a61-c961-47c1-80fb-8e5bd9c957bc\" alt=\"104946561_0_final\" width=\"45%\">\n",
        "    <img src=\"https://github.com/siwei-io/talks/assets/1651790/e587d229-3973-4a3a-856e-0b493ad690eb\" alt=\"104946743_0_final\" width=\"45%\">\n",
        "</div>\n",
        "\n",
        "> Takeaway: KG reasons things reasonably, as it holds the domain knowledge.\n"
      ],
      "metadata": {
        "id": "Q4QMkKKTumXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_rag_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "0E9hJxXdKJdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector_rag = vector_rag_query_engine.query(\n",
        "\"\"\"\n",
        "during their mission on Counter-Earth, the Guardians encounter a mysterious artifact known as the 'Celestial Compass', said to be a relic from Star-Lord's Celestial lineage. Who among the Guardians was tempted to use its power for personal gain?\n",
        "\"\"\"\n",
        ")\n",
        "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
      ],
      "metadata": {
        "id": "XzPofAelKbdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_graph_rag = kg_index_query_engine.query(\n",
        "\"\"\"\n",
        "during their mission on Counter-Earth, the Guardians encounter a mysterious artifact known as the 'Celestial Compass', said to be a relic from Star-Lord's Celestial lineage. Who among the Guardians was tempted to use its power for personal gain?\n",
        "\"\"\"\n",
        ")\n",
        "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
      ],
      "metadata": {
        "id": "gxsrWUi4NLFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backup runtime contexts\n",
        "#!zip -r workshop_dump.zip openrc storage_graph storage_vector"
      ],
      "metadata": {
        "id": "4H3MvUq_KP6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restore runtime contexts\n",
        "!unzip workshop_dump.zip"
      ],
      "metadata": {
        "id": "UGIVzm9IE7Hz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}